{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading region bounding boxes for computing carbon emissions region, this may take a moment...\n",
      " 454/454... rate=467.44 Hz, eta=0:00:00, total=0:00:00, wall=15:37 CETT\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from classify_attention_patterns import load_model\n",
    "from argparse import Namespace\n",
    "from run_glue import load_and_cache_examples, set_seed\n",
    "from model_bert import BertForSequenceClassification, BertForMaskedLM\n",
    "from config_bert import BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_classifier_model, label2id, min_max_size = load_model(\"../models/head_classifier/classify_attention_patters.tar\")\n",
    "head_classifier_model = head_classifier_model.eval().cuda()\n",
    "id2label = {idx:label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned model - Super Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA\n",
      "attention: {'other': 0.4611764705882353, 'mix': 0.24670588235294116, 'vertical': 0.10911764705882353, 'diagonal': 0.09170588235294118, 'block': 0.09129411764705882}\n",
      "weight normed: {'block': 0.789235294117647, 'diagonal': 0.11829411764705883, 'other': 0.07758823529411765, 'mix': 0.01488235294117647}\n",
      "SST-2\n",
      "attention: {'other': 0.5504210526315789, 'block': 0.2391578947368421, 'diagonal': 0.11105263157894738, 'mix': 0.08294736842105263, 'vertical': 0.016421052631578947}\n",
      "weight normed: {'block': 0.798, 'other': 0.152, 'diagonal': 0.040736842105263155, 'mix': 0.009263157894736843}\n",
      "MRPC\n",
      "attention: {'vertical': 0.39321428571428574, 'other': 0.24957142857142858, 'block': 0.21857142857142858, 'mix': 0.06992857142857142, 'diagonal': 0.06871428571428571}\n",
      "weight normed: {'block': 0.9432142857142857, 'diagonal': 0.03578571428571429, 'other': 0.020785714285714286, 'mix': 0.00021428571428571427}\n",
      "STS-B\n",
      "attention: {'other': 0.6716666666666666, 'block': 0.24308333333333335, 'diagonal': 0.0325, 'mix': 0.027416666666666666, 'vertical': 0.025333333333333333}\n",
      "weight normed: {'block': 0.903, 'other': 0.09641666666666666, 'vertical': 0.0005, 'mix': 8.333333333333333e-05}\n",
      "QQP\n",
      "attention: {'other': 0.6066086956521739, 'vertical': 0.18513043478260868, 'mix': 0.09695652173913044, 'block': 0.05782608695652174, 'diagonal': 0.05347826086956522}\n",
      "weight normed: {'block': 0.7673913043478261, 'other': 0.19278260869565217, 'diagonal': 0.036695652173913046, 'mix': 0.003130434782608696}\n",
      "MNLI\n",
      "attention: {'other': 0.48205, 'block': 0.308, 'diagonal': 0.1197, 'mix': 0.06825, 'vertical': 0.022}\n",
      "weight normed: {'block': 0.8448, 'other': 0.1184, 'diagonal': 0.03505, 'mix': 0.00175}\n",
      "QNLI\n",
      "attention: {'other': 0.44941935483870965, 'block': 0.21283870967741936, 'diagonal': 0.15812903225806452, 'mix': 0.10393548387096774, 'vertical': 0.07567741935483871}\n",
      "weight normed: {'block': 0.9229677419354839, 'other': 0.05296774193548387, 'diagonal': 0.022193548387096775, 'mix': 0.0018709677419354838}\n",
      "RTE\n",
      "attention: {'block': 0.47363636363636363, 'other': 0.35654545454545455, 'mix': 0.07872727272727273, 'diagonal': 0.05545454545454546, 'vertical': 0.03563636363636364}\n",
      "weight normed: {'block': 0.9481818181818182, 'other': 0.050727272727272725, 'mix': 0.0007272727272727272, 'diagonal': 0.0003636363636363636}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    attention_counter = Counter()\n",
    "    normed_attention_counter = Counter()\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained(model_path,  config=config)\n",
    "        # Prune\n",
    "        mask_path = f\"../masks/heads_mlps_super/{task}/{seed}/\"\n",
    "        head_mask = np.load(f\"{mask_path}/head_mask.npy\")\n",
    "        mlp_mask = np.load(f\"{mask_path}/mlp_mask.npy\")\n",
    "        head_mask = torch.from_numpy(head_mask)\n",
    "        heads_to_prune = {} \n",
    "        for layer in range(len(head_mask)):\n",
    "            heads_to_mask = [h[0] for h in (1 - head_mask[layer].long()).nonzero().tolist()]\n",
    "            heads_to_prune[layer] = heads_to_mask\n",
    "        mlps_to_prune = [h[0] for h in (1 - torch.from_numpy(mlp_mask).long()).nonzero().tolist()]\n",
    "\n",
    "        transformer_model.prune_heads(heads_to_prune)\n",
    "        transformer_model.prune_mlps(mlps_to_prune)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        attention_head_types = [[] for _ in range(12)]\n",
    "        normed_attention_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            n_tokens = batch[1].sum()\n",
    "            if n_tokens < min_max_size[0]:\n",
    "                continue\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            with torch.no_grad():\n",
    "                _, _, attentions, allalpha_f, attention_mask = transformer_model(**input_data)\n",
    "                for layer in range(len(attentions)):\n",
    "                    if attentions[layer] is None:\n",
    "                        continue\n",
    "\n",
    "                    num_classifier_tokens = max(n_tokens, min_max_size[0]+20)\n",
    "                    head_attentions = attentions[layer].transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        attention_head_types[layer][i][label] += 1\n",
    "\n",
    "                    normed_alpha_f = allalpha_f[layer].norm(dim=-1)\n",
    "                    weight_normed_attention = (normed_alpha_f+attention_mask).softmax(dim=-1)\n",
    "                    head_attentions = weight_normed_attention.transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(normed_attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            normed_attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        normed_attention_head_types[layer][i][label] += 1\n",
    "                del attentions, allalpha_f, attention_mask\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        for layer in attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                attention_counter += head_type_ctr\n",
    "        total_counter = Counter()\n",
    "        for layer in normed_attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                normed_attention_counter += head_type_ctr\n",
    "    print(task)\n",
    "    print(\"attention:\", {k:v/sum(attention_counter.values()) for k,v in attention_counter.most_common()})\n",
    "    print(\"weight normed:\", {k:v/sum(normed_attention_counter.values()) for k,v in normed_attention_counter.most_common()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Model - All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA\n",
      "attention: {'other': 0.4894027777777778, 'vertical': 0.229875, 'mix': 0.16377777777777777, 'block': 0.065125, 'diagonal': 0.051819444444444446}\n",
      "weight normed: {'block': 0.8005416666666667, 'other': 0.1351527777777778, 'diagonal': 0.04770833333333333, 'mix': 0.01622222222222222, 'vertical': 0.000375}\n",
      "SST-2\n",
      "attention: {'other': 0.3145833333333333, 'mix': 0.30145833333333333, 'vertical': 0.21038888888888888, 'diagonal': 0.08898611111111111, 'block': 0.08458333333333333}\n",
      "weight normed: {'block': 0.6183611111111111, 'other': 0.2445277777777778, 'diagonal': 0.077875, 'mix': 0.058375, 'vertical': 0.0008611111111111111}\n",
      "MRPC\n",
      "attention: {'other': 0.29868055555555556, 'mix': 0.2895277777777778, 'vertical': 0.233375, 'diagonal': 0.11504166666666667, 'block': 0.063375}\n",
      "weight normed: {'block': 0.7511666666666666, 'other': 0.14125, 'diagonal': 0.07773611111111112, 'mix': 0.028819444444444446, 'vertical': 0.0010277777777777778}\n",
      "STS-B\n",
      "attention: {'other': 0.5713055555555555, 'mix': 0.16594444444444445, 'vertical': 0.12705555555555556, 'diagonal': 0.07665277777777778, 'block': 0.059041666666666666}\n",
      "weight normed: {'block': 0.7593888888888889, 'other': 0.1421388888888889, 'diagonal': 0.06616666666666667, 'mix': 0.03179166666666667, 'vertical': 0.0005138888888888889}\n",
      "QQP\n",
      "attention: {'other': 0.6490277777777778, 'mix': 0.18263888888888888, 'diagonal': 0.06711111111111111, 'vertical': 0.05619444444444444, 'block': 0.04502777777777778}\n",
      "weight normed: {'block': 0.6055694444444445, 'other': 0.26545833333333335, 'diagonal': 0.06661111111111111, 'mix': 0.06090277777777778, 'vertical': 0.0014583333333333334}\n",
      "MNLI\n",
      "attention: {'other': 0.397125, 'mix': 0.3285, 'vertical': 0.14145833333333332, 'diagonal': 0.07580555555555556, 'block': 0.05711111111111111}\n",
      "weight normed: {'block': 0.6176527777777778, 'other': 0.24566666666666667, 'mix': 0.06954166666666667, 'diagonal': 0.06576388888888889, 'vertical': 0.001375}\n",
      "QNLI\n",
      "attention: {'other': 0.525375, 'mix': 0.20427777777777778, 'diagonal': 0.09605555555555556, 'block': 0.09284722222222222, 'vertical': 0.08144444444444444}\n",
      "weight normed: {'block': 0.6373333333333333, 'other': 0.2400138888888889, 'diagonal': 0.06822222222222223, 'mix': 0.05225, 'vertical': 0.0021805555555555554}\n",
      "RTE\n",
      "attention: {'mix': 0.36, 'other': 0.2449861111111111, 'vertical': 0.20183333333333334, 'diagonal': 0.09773611111111111, 'block': 0.09544444444444444}\n",
      "weight normed: {'block': 0.7426805555555556, 'other': 0.16884722222222223, 'diagonal': 0.06176388888888889, 'mix': 0.02384722222222222, 'vertical': 0.002861111111111111}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    attention_counter = Counter()\n",
    "    normed_attention_counter = Counter()\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained(model_path,  config=config)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        attention_head_types = [[] for _ in range(12)]\n",
    "        normed_attention_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            n_tokens = batch[1].sum()\n",
    "            if n_tokens < min_max_size[0]:\n",
    "                continue\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            with torch.no_grad():\n",
    "                _, _, attentions, allalpha_f, attention_mask = transformer_model(**input_data)\n",
    "                for layer in range(len(attentions)):\n",
    "                    if attentions[layer] is None:\n",
    "                        continue\n",
    "\n",
    "                    num_classifier_tokens = max(n_tokens, min_max_size[0]+20)\n",
    "                    head_attentions = attentions[layer].transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        attention_head_types[layer][i][label] += 1\n",
    "\n",
    "                    normed_alpha_f = allalpha_f[layer].norm(dim=-1)\n",
    "                    weight_normed_attention = (normed_alpha_f+attention_mask).softmax(dim=-1)\n",
    "                    head_attentions = weight_normed_attention.transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(normed_attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            normed_attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        normed_attention_head_types[layer][i][label] += 1\n",
    "                del attentions, allalpha_f, attention_mask\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        for layer in attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                attention_counter += head_type_ctr\n",
    "        total_counter = Counter()\n",
    "        for layer in normed_attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                normed_attention_counter += head_type_ctr\n",
    "    print(task)\n",
    "    print(\"attention:\", {k:v/sum(attention_counter.values()) for k,v in attention_counter.most_common()})\n",
    "    print(\"weight normed:\", {k:v/sum(normed_attention_counter.values()) for k,v in normed_attention_counter.most_common()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Super Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA\n",
      "attention: {'other': 0.4672352941176471, 'mix': 0.26794117647058824, 'vertical': 0.10170588235294117, 'diagonal': 0.0838235294117647, 'block': 0.07929411764705882}\n",
      "weight normed: {'block': 0.7966470588235294, 'diagonal': 0.10970588235294118, 'other': 0.08041176470588235, 'mix': 0.013235294117647059}\n",
      "SST-2\n",
      "attention: {'other': 0.5573684210526316, 'block': 0.23326315789473684, 'diagonal': 0.11168421052631579, 'mix': 0.07810526315789473, 'vertical': 0.019578947368421053}\n",
      "weight normed: {'block': 0.8006315789473685, 'other': 0.1491578947368421, 'diagonal': 0.04347368421052632, 'mix': 0.006736842105263158}\n",
      "MRPC\n",
      "attention: {'vertical': 0.40035714285714286, 'other': 0.2547857142857143, 'block': 0.21471428571428572, 'diagonal': 0.06785714285714285, 'mix': 0.062285714285714285}\n",
      "weight normed: {'block': 0.9435714285714286, 'diagonal': 0.03578571428571429, 'other': 0.020428571428571428, 'mix': 0.00021428571428571427}\n",
      "STS-B\n",
      "attention: {'other': 0.6811666666666667, 'block': 0.22441666666666665, 'diagonal': 0.03275, 'vertical': 0.032, 'mix': 0.029666666666666668}\n",
      "weight normed: {'block': 0.9040833333333333, 'other': 0.09533333333333334, 'vertical': 0.0005, 'mix': 8.333333333333333e-05}\n",
      "QQP\n",
      "attention: {'other': 0.6028695652173913, 'vertical': 0.19695652173913045, 'mix': 0.08356521739130435, 'diagonal': 0.058434782608695654, 'block': 0.05817391304347826}\n",
      "weight normed: {'block': 0.8042608695652174, 'other': 0.15565217391304348, 'diagonal': 0.038, 'mix': 0.0020869565217391303}\n",
      "MNLI\n",
      "attention: {'other': 0.5534, 'block': 0.21675, 'diagonal': 0.10575, 'mix': 0.0628, 'vertical': 0.0613}\n",
      "weight normed: {'block': 0.84155, 'other': 0.12455, 'diagonal': 0.0328, 'mix': 0.0011}\n",
      "QNLI\n",
      "attention: {'other': 0.4567741935483871, 'block': 0.212, 'diagonal': 0.15116129032258063, 'mix': 0.09070967741935484, 'vertical': 0.08935483870967742}\n",
      "weight normed: {'block': 0.9392903225806452, 'other': 0.03896774193548387, 'diagonal': 0.02064516129032258, 'mix': 0.0010967741935483872}\n",
      "RTE\n",
      "attention: {'block': 0.4647272727272727, 'other': 0.3643636363636364, 'mix': 0.08181818181818182, 'diagonal': 0.054, 'vertical': 0.03509090909090909}\n",
      "weight normed: {'block': 0.9463636363636364, 'other': 0.05272727272727273, 'mix': 0.0005454545454545455, 'diagonal': 0.0003636363636363636}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    attention_counter = Counter()\n",
    "    normed_attention_counter = Counter()\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',  config=config)\n",
    "        # Prune\n",
    "        mask_path = f\"../masks/heads_mlps_super/{task}/{seed}/\"\n",
    "        head_mask = np.load(f\"{mask_path}/head_mask.npy\")\n",
    "        mlp_mask = np.load(f\"{mask_path}/mlp_mask.npy\")\n",
    "        head_mask = torch.from_numpy(head_mask)\n",
    "        heads_to_prune = {} \n",
    "        for layer in range(len(head_mask)):\n",
    "            heads_to_mask = [h[0] for h in (1 - head_mask[layer].long()).nonzero().tolist()]\n",
    "            heads_to_prune[layer] = heads_to_mask\n",
    "        mlps_to_prune = [h[0] for h in (1 - torch.from_numpy(mlp_mask).long()).nonzero().tolist()]\n",
    "\n",
    "        transformer_model.prune_heads(heads_to_prune)\n",
    "        transformer_model.prune_mlps(mlps_to_prune)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        attention_head_types = [[] for _ in range(12)]\n",
    "        normed_attention_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            n_tokens = batch[1].sum()\n",
    "            if n_tokens < min_max_size[0]:\n",
    "                continue\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            with torch.no_grad():\n",
    "                _, _, attentions, allalpha_f, attention_mask = transformer_model(**input_data)\n",
    "                for layer in range(len(attentions)):\n",
    "                    if attentions[layer] is None:\n",
    "                        continue\n",
    "\n",
    "                    num_classifier_tokens = max(n_tokens, min_max_size[0]+20)\n",
    "                    head_attentions = attentions[layer].transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        attention_head_types[layer][i][label] += 1\n",
    "\n",
    "                    normed_alpha_f = allalpha_f[layer].norm(dim=-1)\n",
    "                    weight_normed_attention = (normed_alpha_f+attention_mask).softmax(dim=-1)\n",
    "                    head_attentions = weight_normed_attention.transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(normed_attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            normed_attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        normed_attention_head_types[layer][i][label] += 1\n",
    "                del attentions, allalpha_f, attention_mask\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        for layer in attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                attention_counter += head_type_ctr\n",
    "        total_counter = Counter()\n",
    "        for layer in normed_attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                normed_attention_counter += head_type_ctr\n",
    "    print(task)\n",
    "    print(\"attention:\", {k:v/sum(attention_counter.values()) for k,v in attention_counter.most_common()})\n",
    "    print(\"weight normed:\", {k:v/sum(normed_attention_counter.values()) for k,v in normed_attention_counter.most_common()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA\n",
      "attention: {'other': 0.48944444444444446, 'vertical': 0.26025, 'mix': 0.14822222222222223, 'block': 0.05431944444444444, 'diagonal': 0.04776388888888889}\n",
      "weight normed: {'block': 0.8306111111111111, 'other': 0.11386111111111111, 'diagonal': 0.043166666666666666, 'mix': 0.012319444444444444, 'vertical': 4.1666666666666665e-05}\n",
      "SST-2\n",
      "attention: {'other': 0.3076527777777778, 'mix': 0.27318055555555554, 'vertical': 0.25622222222222224, 'diagonal': 0.08556944444444445, 'block': 0.077375}\n",
      "weight normed: {'block': 0.6592638888888889, 'other': 0.2195138888888889, 'diagonal': 0.07645833333333334, 'mix': 0.04452777777777778, 'vertical': 0.00023611111111111112}\n",
      "MRPC\n",
      "attention: {'other': 0.29683333333333334, 'vertical': 0.2928611111111111, 'mix': 0.2599722222222222, 'diagonal': 0.09270833333333334, 'block': 0.057625}\n",
      "weight normed: {'block': 0.7734861111111111, 'other': 0.13669444444444445, 'diagonal': 0.06644444444444444, 'mix': 0.022319444444444444, 'vertical': 0.0010555555555555555}\n",
      "STS-B\n",
      "attention: {'other': 0.6141527777777778, 'mix': 0.1490138888888889, 'vertical': 0.12743055555555555, 'diagonal': 0.06420833333333334, 'block': 0.04519444444444445}\n",
      "weight normed: {'block': 0.7793472222222222, 'other': 0.14002777777777778, 'diagonal': 0.05720833333333333, 'mix': 0.023069444444444445, 'vertical': 0.00034722222222222224}\n",
      "QQP\n",
      "attention: {'other': 0.66475, 'mix': 0.14891666666666667, 'vertical': 0.09115277777777778, 'diagonal': 0.06327777777777778, 'block': 0.03190277777777778}\n",
      "weight normed: {'block': 0.6944583333333333, 'other': 0.20727777777777778, 'diagonal': 0.06377777777777778, 'mix': 0.03422222222222222, 'vertical': 0.00026388888888888886}\n",
      "MNLI\n",
      "attention: {'other': 0.42259722222222224, 'mix': 0.25834722222222223, 'vertical': 0.19569444444444445, 'diagonal': 0.07738888888888888, 'block': 0.04597222222222222}\n",
      "weight normed: {'block': 0.7109861111111111, 'other': 0.19118055555555555, 'diagonal': 0.06518055555555556, 'mix': 0.03219444444444444, 'vertical': 0.0004583333333333333}\n",
      "QNLI\n",
      "attention: {'other': 0.5500277777777778, 'mix': 0.14444444444444443, 'vertical': 0.12756944444444446, 'diagonal': 0.09130555555555556, 'block': 0.08665277777777777}\n",
      "weight normed: {'block': 0.7233194444444444, 'other': 0.18316666666666667, 'diagonal': 0.06273611111111112, 'mix': 0.029472222222222223, 'vertical': 0.0013055555555555555}\n",
      "RTE\n",
      "attention: {'mix': 0.31604166666666667, 'vertical': 0.257875, 'other': 0.23683333333333334, 'block': 0.09572222222222222, 'diagonal': 0.09352777777777778}\n",
      "weight normed: {'block': 0.7675, 'other': 0.15231944444444445, 'diagonal': 0.057277777777777775, 'mix': 0.02047222222222222, 'vertical': 0.0024305555555555556}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    attention_counter = Counter()\n",
    "    normed_attention_counter = Counter()\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',  config=config)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        attention_head_types = [[] for _ in range(12)]\n",
    "        normed_attention_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            n_tokens = batch[1].sum()\n",
    "            if n_tokens < min_max_size[0]:\n",
    "                continue\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            with torch.no_grad():\n",
    "                _, _, attentions, allalpha_f, attention_mask = transformer_model(**input_data)\n",
    "                for layer in range(len(attentions)):\n",
    "                    if attentions[layer] is None:\n",
    "                        continue\n",
    "\n",
    "                    num_classifier_tokens = max(n_tokens, min_max_size[0]+20)\n",
    "                    head_attentions = attentions[layer].transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        attention_head_types[layer][i][label] += 1\n",
    "\n",
    "                    normed_alpha_f = allalpha_f[layer].norm(dim=-1)\n",
    "                    weight_normed_attention = (normed_alpha_f+attention_mask).softmax(dim=-1)\n",
    "                    head_attentions = weight_normed_attention.transpose(0, 1)\n",
    "\n",
    "                    head_attentions = head_attentions[:,:,:num_classifier_tokens,:num_classifier_tokens]\n",
    "                    logits = head_classifier_model(head_attentions)\n",
    "                    label_ids = torch.argmax(logits, dim=-1)\n",
    "                    labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                    if len(normed_attention_head_types[layer]) == 0:\n",
    "                        for i in range(len(labels)):\n",
    "                            normed_attention_head_types[layer].append(Counter())\n",
    "                    for i, label in enumerate(labels):\n",
    "                        normed_attention_head_types[layer][i][label] += 1\n",
    "                del attentions, allalpha_f, attention_mask\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        for layer in attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                attention_counter += head_type_ctr\n",
    "        total_counter = Counter()\n",
    "        for layer in normed_attention_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                normed_attention_counter += head_type_ctr\n",
    "    print(task)\n",
    "    print(\"attention:\", {k:v/sum(attention_counter.values()) for k,v in attention_counter.most_common()})\n",
    "    print(\"weight normed:\", {k:v/sum(normed_attention_counter.values()) for k,v in normed_attention_counter.most_common()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
