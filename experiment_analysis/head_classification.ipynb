{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading region bounding boxes for computing carbon emissions region, this may take a moment...\n",
      " 454/454... rate=538.78 Hz, eta=0:00:00, total=0:00:00, wall=22:02 CETT\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from classify_attention_patterns import load_model\n",
    "from argparse import Namespace\n",
    "from run_glue import load_and_cache_examples, set_seed\n",
    "from model_bert import BertForSequenceClassification\n",
    "from config_bert import BertConfig\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_classifier_model, label2id, min_max_size = load_model(\"../models/head_classifier/classify_attention_patters.tar\")\n",
    "head_classifier_model = head_classifier_model.eval().cuda()\n",
    "id2label = {idx:label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned model - Super Survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA seed_1337 {'vertical': 0.42764705882352944, 'block': 0.4047058823529412, 'other': 0.14941176470588236, 'mix': 0.012941176470588235, 'diagonal': 0.005294117647058823}\n",
      "CoLA seed_42 {'block': 0.4364705882352941, 'vertical': 0.39705882352941174, 'other': 0.1426470588235294, 'mix': 0.016764705882352942, 'diagonal': 0.007058823529411765}\n",
      "CoLA seed_86 {'block': 0.4370588235294118, 'vertical': 0.4117647058823529, 'other': 0.14588235294117646, 'mix': 0.003823529411764706, 'diagonal': 0.0014705882352941176}\n",
      "CoLA seed_71 {'vertical': 0.4485294117647059, 'block': 0.4041176470588235, 'other': 0.14088235294117646, 'mix': 0.005294117647058823, 'diagonal': 0.001176470588235294}\n",
      "CoLA seed_166 {'block': 0.43911764705882356, 'vertical': 0.3905882352941176, 'other': 0.1561764705882353, 'mix': 0.01, 'diagonal': 0.00411764705882353}\n",
      "SST-2 seed_1337 {'other': 0.5178947368421053, 'block': 0.22421052631578947, 'vertical': 0.11105263157894738, 'mix': 0.09, 'diagonal': 0.056842105263157895}\n",
      "SST-2 seed_42 {'other': 0.511578947368421, 'block': 0.2668421052631579, 'vertical': 0.10578947368421053, 'mix': 0.06526315789473684, 'diagonal': 0.05052631578947368}\n",
      "SST-2 seed_86 {'other': 0.5163157894736842, 'block': 0.23421052631578948, 'vertical': 0.11, 'mix': 0.08263157894736842, 'diagonal': 0.056842105263157895}\n",
      "SST-2 seed_71 {'other': 0.5110526315789473, 'block': 0.2336842105263158, 'vertical': 0.1036842105263158, 'mix': 0.0868421052631579, 'diagonal': 0.06473684210526316}\n",
      "SST-2 seed_166 {'other': 0.5136842105263157, 'block': 0.22052631578947368, 'vertical': 0.11473684210526315, 'mix': 0.0863157894736842, 'diagonal': 0.06473684210526316}\n",
      "MRPC seed_1337 {'vertical': 0.5217857142857143, 'other': 0.2282142857142857, 'block': 0.18678571428571428, 'diagonal': 0.03571428571428571, 'mix': 0.0275}\n",
      "MRPC seed_42 {'vertical': 0.5271428571428571, 'other': 0.22392857142857142, 'block': 0.18357142857142858, 'diagonal': 0.036071428571428574, 'mix': 0.029285714285714286}\n",
      "MRPC seed_86 {'vertical': 0.5117857142857143, 'other': 0.23464285714285715, 'block': 0.17642857142857143, 'mix': 0.04107142857142857, 'diagonal': 0.036071428571428574}\n",
      "MRPC seed_71 {'vertical': 0.5107142857142857, 'other': 0.22892857142857143, 'block': 0.18785714285714286, 'mix': 0.03678571428571428, 'diagonal': 0.03571428571428571}\n",
      "MRPC seed_166 {'vertical': 0.49714285714285716, 'other': 0.22464285714285714, 'block': 0.18785714285714286, 'mix': 0.054285714285714284, 'diagonal': 0.036071428571428574}\n",
      "STS-B seed_1337 {'other': 0.6666666666666666, 'block': 0.24, 'vertical': 0.07291666666666667, 'mix': 0.013333333333333334, 'diagonal': 0.007083333333333333}\n",
      "STS-B seed_42 {'other': 0.6204166666666666, 'block': 0.265, 'vertical': 0.08125, 'mix': 0.021666666666666667, 'diagonal': 0.011666666666666667}\n",
      "STS-B seed_86 {'other': 0.6570833333333334, 'block': 0.23125, 'vertical': 0.08458333333333333, 'mix': 0.02125, 'diagonal': 0.005833333333333334}\n",
      "STS-B seed_71 {'other': 0.62875, 'block': 0.24291666666666667, 'vertical': 0.1025, 'mix': 0.020416666666666666, 'diagonal': 0.005416666666666667}\n",
      "STS-B seed_166 {'other': 0.6466666666666666, 'block': 0.245, 'vertical': 0.08166666666666667, 'mix': 0.0175, 'diagonal': 0.009166666666666667}\n",
      "QQP seed_1337 {'other': 0.5265217391304348, 'vertical': 0.3560869565217391, 'mix': 0.06739130434782609, 'block': 0.036521739130434785, 'diagonal': 0.013478260869565217}\n",
      "QQP seed_42 {'other': 0.5952173913043478, 'vertical': 0.28695652173913044, 'mix': 0.059130434782608696, 'block': 0.04739130434782609, 'diagonal': 0.011304347826086957}\n",
      "QQP seed_86 {'other': 0.5986956521739131, 'vertical': 0.27695652173913043, 'mix': 0.07695652173913044, 'block': 0.03304347826086956, 'diagonal': 0.014347826086956521}\n",
      "QQP seed_71 {'other': 0.6295652173913043, 'vertical': 0.26260869565217393, 'mix': 0.05304347826086957, 'block': 0.03869565217391304, 'diagonal': 0.01608695652173913}\n",
      "QQP seed_166 {'other': 0.5947826086956521, 'vertical': 0.2952173913043478, 'mix': 0.05347826086956522, 'block': 0.0391304347826087, 'diagonal': 0.017391304347826087}\n",
      "MNLI seed_1337 {'block': 0.49975, 'other': 0.40925, 'diagonal': 0.0665, 'vertical': 0.01925, 'mix': 0.00525}\n",
      "MNLI seed_42 {'block': 0.47975, 'other': 0.43175, 'diagonal': 0.0635, 'vertical': 0.01975, 'mix': 0.00525}\n",
      "MNLI seed_86 {'block': 0.49175, 'other': 0.42025, 'diagonal': 0.0745, 'vertical': 0.008, 'mix': 0.0055}\n",
      "MNLI seed_71 {'block': 0.47825, 'other': 0.43225, 'diagonal': 0.0755, 'vertical': 0.00975, 'mix': 0.00425}\n",
      "MNLI seed_166 {'block': 0.46575, 'other': 0.44075, 'diagonal': 0.0675, 'vertical': 0.01775, 'mix': 0.00825}\n",
      "QNLI seed_1337 {'vertical': 0.3193548387096774, 'other': 0.31870967741935485, 'block': 0.15483870967741936, 'mix': 0.13870967741935483, 'diagonal': 0.06838709677419355}\n",
      "QNLI seed_42 {'other': 0.38483870967741934, 'vertical': 0.2787096774193548, 'block': 0.1503225806451613, 'mix': 0.11419354838709678, 'diagonal': 0.07193548387096774}\n",
      "QNLI seed_86 {'other': 0.36193548387096774, 'vertical': 0.2938709677419355, 'block': 0.14096774193548386, 'mix': 0.13903225806451613, 'diagonal': 0.06419354838709677}\n",
      "QNLI seed_71 {'other': 0.3961290322580645, 'vertical': 0.24774193548387097, 'block': 0.15645161290322582, 'mix': 0.12096774193548387, 'diagonal': 0.07870967741935483}\n",
      "QNLI seed_166 {'other': 0.3329032258064516, 'vertical': 0.31709677419354837, 'block': 0.15483870967741936, 'mix': 0.12548387096774194, 'diagonal': 0.0696774193548387}\n",
      "RTE seed_1337 {'other': 0.5372727272727272, 'block': 0.34, 'mix': 0.07090909090909091, 'vertical': 0.03909090909090909, 'diagonal': 0.012727272727272728}\n",
      "RTE seed_42 {'other': 0.49, 'block': 0.4027272727272727, 'mix': 0.06272727272727273, 'diagonal': 0.023636363636363636, 'vertical': 0.02090909090909091}\n",
      "RTE seed_86 {'other': 0.46090909090909093, 'block': 0.4163636363636364, 'mix': 0.06636363636363636, 'vertical': 0.03363636363636364, 'diagonal': 0.022727272727272728}\n",
      "RTE seed_71 {'other': 0.4954545454545455, 'block': 0.3972727272727273, 'mix': 0.060909090909090906, 'diagonal': 0.023636363636363636, 'vertical': 0.022727272727272728}\n",
      "RTE seed_166 {'other': 0.4818181818181818, 'block': 0.3981818181818182, 'mix': 0.07272727272727272, 'vertical': 0.03090909090909091, 'diagonal': 0.016363636363636365}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained(model_path,  config=config)\n",
    "        # Prune\n",
    "        mask_path = f\"../masks/heads_mlps_super/{task}/{seed}/\"\n",
    "        head_mask = np.load(f\"{mask_path}/head_mask.npy\")\n",
    "        mlp_mask = np.load(f\"{mask_path}/mlp_mask.npy\")\n",
    "        head_mask = torch.from_numpy(head_mask)\n",
    "        heads_to_prune = {} \n",
    "        for layer in range(len(head_mask)):\n",
    "            heads_to_mask = [h[0] for h in (1 - head_mask[layer].long()).nonzero().tolist()]\n",
    "            heads_to_prune[layer] = heads_to_mask\n",
    "        mlps_to_prune = [h[0] for h in (1 - torch.from_numpy(mlp_mask).long()).nonzero().tolist()]\n",
    "\n",
    "        transformer_model.prune_heads(heads_to_prune)\n",
    "        transformer_model.prune_mlps(mlps_to_prune)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        layer_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            _, _, attentions = transformer_model(**input_data)\n",
    "            for layer in range(len(attentions)):\n",
    "                if attentions[layer] is None:\n",
    "                    continue\n",
    "                head_attentions = attentions[layer].transpose(0, 1)\n",
    "                logits = head_classifier_model(head_attentions)\n",
    "                label_ids = torch.argmax(logits, dim=-1)\n",
    "                labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                if len(layer_head_types[layer]) == 0:\n",
    "                    for i in range(len(labels)):\n",
    "                        c = Counter()\n",
    "                        layer_head_types[layer].append(c)\n",
    "                for i, label in enumerate(labels):\n",
    "                    layer_head_types[layer][i][label] += 1\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        total_counter = Counter()\n",
    "        for layer in layer_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                total_counter += head_type_ctr\n",
    "        print(task, seed, {k:v/sum(total_counter.values()) for k,v in total_counter.most_common()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model - Super-survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA seed_1337 {'vertical': 0.4202941176470588, 'block': 0.4047058823529412, 'other': 0.15911764705882353, 'mix': 0.011176470588235295, 'diagonal': 0.004705882352941176}\n",
      "CoLA seed_42 {'block': 0.42764705882352944, 'vertical': 0.4111764705882353, 'other': 0.13970588235294118, 'mix': 0.015588235294117648, 'diagonal': 0.0058823529411764705}\n",
      "CoLA seed_86 {'vertical': 0.43441176470588233, 'block': 0.40941176470588236, 'other': 0.15088235294117647, 'mix': 0.003823529411764706, 'diagonal': 0.0014705882352941176}\n",
      "CoLA seed_71 {'vertical': 0.4576470588235294, 'block': 0.39588235294117646, 'other': 0.14029411764705882, 'mix': 0.005, 'diagonal': 0.001176470588235294}\n",
      "CoLA seed_166 {'vertical': 0.43441176470588233, 'block': 0.4023529411764706, 'other': 0.15088235294117647, 'mix': 0.008529411764705883, 'diagonal': 0.003823529411764706}\n",
      "SST-2 seed_1337 {'other': 0.5194736842105263, 'block': 0.2168421052631579, 'vertical': 0.12, 'mix': 0.09368421052631579, 'diagonal': 0.05}\n",
      "SST-2 seed_42 {'other': 0.5236842105263158, 'block': 0.25, 'vertical': 0.11578947368421053, 'mix': 0.06789473684210526, 'diagonal': 0.04263157894736842}\n",
      "SST-2 seed_86 {'other': 0.52, 'block': 0.2326315789473684, 'vertical': 0.11473684210526315, 'mix': 0.08, 'diagonal': 0.05263157894736842}\n",
      "SST-2 seed_71 {'other': 0.5184210526315789, 'block': 0.22263157894736843, 'vertical': 0.1168421052631579, 'mix': 0.0863157894736842, 'diagonal': 0.05578947368421053}\n",
      "SST-2 seed_166 {'other': 0.521578947368421, 'block': 0.21789473684210525, 'vertical': 0.11842105263157894, 'mix': 0.08578947368421053, 'diagonal': 0.05631578947368421}\n",
      "MRPC seed_1337 {'vertical': 0.5467857142857143, 'other': 0.22642857142857142, 'block': 0.18571428571428572, 'diagonal': 0.03571428571428571, 'mix': 0.005357142857142857}\n",
      "MRPC seed_42 {'vertical': 0.5503571428571429, 'other': 0.22678571428571428, 'block': 0.18142857142857144, 'diagonal': 0.036071428571428574, 'mix': 0.005357142857142857}\n",
      "MRPC seed_86 {'vertical': 0.5503571428571429, 'other': 0.2375, 'block': 0.17, 'diagonal': 0.036071428571428574, 'mix': 0.006071428571428571}\n",
      "MRPC seed_71 {'vertical': 0.5453571428571429, 'other': 0.22678571428571428, 'block': 0.185, 'diagonal': 0.03571428571428571, 'mix': 0.007142857142857143}\n",
      "MRPC seed_166 {'vertical': 0.5435714285714286, 'other': 0.22464285714285714, 'block': 0.18714285714285714, 'diagonal': 0.036071428571428574, 'mix': 0.008571428571428572}\n",
      "STS-B seed_1337 {'other': 0.6625, 'block': 0.23083333333333333, 'vertical': 0.08625, 'mix': 0.01375, 'diagonal': 0.006666666666666667}\n",
      "STS-B seed_42 {'other': 0.6275, 'block': 0.24791666666666667, 'vertical': 0.0925, 'mix': 0.020833333333333332, 'diagonal': 0.01125}\n",
      "STS-B seed_86 {'other': 0.66875, 'block': 0.21041666666666667, 'vertical': 0.095, 'mix': 0.02125, 'diagonal': 0.004583333333333333}\n",
      "STS-B seed_71 {'other': 0.6395833333333333, 'block': 0.22541666666666665, 'vertical': 0.10916666666666666, 'mix': 0.019583333333333335, 'diagonal': 0.00625}\n",
      "STS-B seed_166 {'other': 0.6508333333333334, 'block': 0.23458333333333334, 'vertical': 0.08833333333333333, 'mix': 0.017083333333333332, 'diagonal': 0.009166666666666667}\n",
      "QQP seed_1337 {'other': 0.5195652173913043, 'vertical': 0.36217391304347823, 'mix': 0.056956521739130433, 'block': 0.043478260869565216, 'diagonal': 0.017826086956521738}\n",
      "QQP seed_42 {'other': 0.5917391304347827, 'vertical': 0.28391304347826085, 'block': 0.06217391304347826, 'mix': 0.04434782608695652, 'diagonal': 0.017826086956521738}\n",
      "QQP seed_86 {'other': 0.5882608695652174, 'vertical': 0.2995652173913044, 'mix': 0.05304347826086957, 'block': 0.03956521739130435, 'diagonal': 0.01956521739130435}\n",
      "QQP seed_71 {'other': 0.6273913043478261, 'vertical': 0.26565217391304347, 'block': 0.04608695652173913, 'mix': 0.044782608695652176, 'diagonal': 0.01608695652173913}\n",
      "QQP seed_166 {'other': 0.6147826086956522, 'vertical': 0.2782608695652174, 'block': 0.04391304347826087, 'mix': 0.043478260869565216, 'diagonal': 0.01956521739130435}\n",
      "MNLI seed_1337 {'other': 0.54225, 'block': 0.3445, 'diagonal': 0.0655, 'vertical': 0.043, 'mix': 0.00475}\n",
      "MNLI seed_42 {'other': 0.536, 'block': 0.35, 'diagonal': 0.06125, 'vertical': 0.04825, 'mix': 0.0045}\n",
      "MNLI seed_86 {'other': 0.5655, 'block': 0.3245, 'diagonal': 0.06975, 'vertical': 0.03425, 'mix': 0.006}\n",
      "MNLI seed_71 {'other': 0.521, 'block': 0.35075, 'diagonal': 0.072, 'vertical': 0.052, 'mix': 0.00425}\n",
      "MNLI seed_166 {'other': 0.5395, 'block': 0.34275, 'diagonal': 0.06375, 'vertical': 0.05, 'mix': 0.004}\n",
      "QNLI seed_1337 {'vertical': 0.35161290322580646, 'other': 0.31193548387096776, 'block': 0.1474193548387097, 'mix': 0.12612903225806452, 'diagonal': 0.06290322580645161}\n",
      "QNLI seed_42 {'other': 0.33903225806451615, 'vertical': 0.3296774193548387, 'block': 0.1435483870967742, 'mix': 0.11903225806451613, 'diagonal': 0.06870967741935484}\n",
      "QNLI seed_86 {'vertical': 0.3606451612903226, 'other': 0.3183870967741935, 'block': 0.13870967741935483, 'mix': 0.11806451612903225, 'diagonal': 0.06419354838709677}\n",
      "QNLI seed_71 {'vertical': 0.3387096774193548, 'other': 0.32645161290322583, 'block': 0.15096774193548387, 'mix': 0.12322580645161291, 'diagonal': 0.06064516129032258}\n",
      "QNLI seed_166 {'vertical': 0.35935483870967744, 'other': 0.30612903225806454, 'block': 0.1461290322580645, 'mix': 0.12387096774193548, 'diagonal': 0.06451612903225806}\n",
      "RTE seed_1337 {'other': 0.5272727272727272, 'block': 0.3609090909090909, 'mix': 0.06727272727272728, 'vertical': 0.02909090909090909, 'diagonal': 0.015454545454545455}\n",
      "RTE seed_42 {'other': 0.5054545454545455, 'block': 0.38181818181818183, 'mix': 0.06636363636363636, 'vertical': 0.02727272727272727, 'diagonal': 0.019090909090909092}\n",
      "RTE seed_86 {'other': 0.45636363636363636, 'block': 0.4209090909090909, 'mix': 0.06272727272727273, 'vertical': 0.035454545454545454, 'diagonal': 0.024545454545454544}\n",
      "RTE seed_71 {'other': 0.5127272727272727, 'block': 0.37636363636363634, 'mix': 0.06272727272727273, 'vertical': 0.028181818181818183, 'diagonal': 0.02}\n",
      "RTE seed_166 {'other': 0.5036363636363637, 'block': 0.3709090909090909, 'mix': 0.07090909090909091, 'vertical': 0.03727272727272727, 'diagonal': 0.017272727272727273}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',  config=config)\n",
    "        # Prune\n",
    "        mask_path = f\"../masks/heads_mlps_super/{task}/{seed}/\"\n",
    "        head_mask = np.load(f\"{mask_path}/head_mask.npy\")\n",
    "        mlp_mask = np.load(f\"{mask_path}/mlp_mask.npy\")\n",
    "        head_mask = torch.from_numpy(head_mask)\n",
    "        heads_to_prune = {} \n",
    "        for layer in range(len(head_mask)):\n",
    "            heads_to_mask = [h[0] for h in (1 - head_mask[layer].long()).nonzero().tolist()]\n",
    "            heads_to_prune[layer] = heads_to_mask\n",
    "        mlps_to_prune = [h[0] for h in (1 - torch.from_numpy(mlp_mask).long()).nonzero().tolist()]\n",
    "\n",
    "        transformer_model.prune_heads(heads_to_prune)\n",
    "        transformer_model.prune_mlps(mlps_to_prune)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        layer_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            _, _, attentions = transformer_model(**input_data)\n",
    "            for layer in range(len(attentions)):\n",
    "                if attentions[layer] is None:\n",
    "                    continue\n",
    "                head_attentions = attentions[layer].transpose(0, 1)\n",
    "                logits = head_classifier_model(head_attentions)\n",
    "                label_ids = torch.argmax(logits, dim=-1)\n",
    "                labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                if len(layer_head_types[layer]) == 0:\n",
    "                    for i in range(len(labels)):\n",
    "                        c = Counter()\n",
    "                        layer_head_types[layer].append(c)\n",
    "                for i, label in enumerate(labels):\n",
    "                    layer_head_types[layer][i][label] += 1\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        total_counter = Counter()\n",
    "        for layer in layer_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                total_counter += head_type_ctr\n",
    "        print(task, seed, {k:v/sum(total_counter.values()) for k,v in total_counter.most_common()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned Model All heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA seed_1337 {'vertical': 0.4676388888888889, 'block': 0.3804166666666667, 'other': 0.14027777777777778, 'mix': 0.009097222222222222, 'diagonal': 0.0025694444444444445}\n",
      "CoLA seed_42 {'vertical': 0.4650694444444444, 'block': 0.3977777777777778, 'other': 0.12104166666666667, 'mix': 0.013472222222222222, 'diagonal': 0.002638888888888889}\n",
      "CoLA seed_86 {'vertical': 0.43166666666666664, 'block': 0.41055555555555556, 'other': 0.14381944444444444, 'mix': 0.010347222222222223, 'diagonal': 0.003611111111111111}\n",
      "CoLA seed_71 {'vertical': 0.44083333333333335, 'block': 0.36993055555555554, 'other': 0.184375, 'mix': 0.0038194444444444443, 'diagonal': 0.0010416666666666667}\n",
      "CoLA seed_166 {'block': 0.4388888888888889, 'vertical': 0.38958333333333334, 'other': 0.1659027777777778, 'mix': 0.004861111111111111, 'diagonal': 0.0007638888888888889}\n",
      "SST-2 seed_1337 {'vertical': 0.32381944444444444, 'other': 0.3097916666666667, 'block': 0.18048611111111112, 'mix': 0.15291666666666667, 'diagonal': 0.03298611111111111}\n",
      "SST-2 seed_42 {'vertical': 0.32243055555555555, 'other': 0.303125, 'block': 0.1701388888888889, 'mix': 0.16854166666666667, 'diagonal': 0.03576388888888889}\n",
      "SST-2 seed_86 {'vertical': 0.32875, 'other': 0.32611111111111113, 'block': 0.16923611111111111, 'mix': 0.15222222222222223, 'diagonal': 0.023680555555555555}\n",
      "SST-2 seed_71 {'vertical': 0.33243055555555556, 'other': 0.296875, 'mix': 0.16854166666666667, 'block': 0.16381944444444443, 'diagonal': 0.03833333333333333}\n",
      "SST-2 seed_166 {'other': 0.3233333333333333, 'vertical': 0.31201388888888887, 'mix': 0.1867361111111111, 'block': 0.13527777777777777, 'diagonal': 0.042638888888888886}\n",
      "MRPC seed_1337 {'other': 0.3090277777777778, 'mix': 0.2775, 'vertical': 0.26513888888888887, 'block': 0.09270833333333334, 'diagonal': 0.055625}\n",
      "MRPC seed_42 {'other': 0.3020138888888889, 'vertical': 0.28069444444444447, 'mix': 0.2551388888888889, 'block': 0.09555555555555556, 'diagonal': 0.06659722222222222}\n",
      "MRPC seed_86 {'other': 0.2915277777777778, 'mix': 0.274375, 'vertical': 0.2611111111111111, 'block': 0.09972222222222223, 'diagonal': 0.07326388888888889}\n",
      "MRPC seed_71 {'mix': 0.29381944444444447, 'other': 0.2785416666666667, 'vertical': 0.2752777777777778, 'block': 0.095625, 'diagonal': 0.05673611111111111}\n",
      "MRPC seed_166 {'other': 0.3426388888888889, 'vertical': 0.2561111111111111, 'mix': 0.23979166666666665, 'block': 0.09451388888888888, 'diagonal': 0.06694444444444445}\n",
      "STS-B seed_1337 {'other': 0.5663888888888889, 'vertical': 0.1338888888888889, 'block': 0.12763888888888889, 'mix': 0.11625, 'diagonal': 0.05583333333333333}\n",
      "STS-B seed_42 {'other': 0.6095138888888889, 'vertical': 0.12680555555555556, 'block': 0.12597222222222224, 'mix': 0.10402777777777777, 'diagonal': 0.033680555555555554}\n",
      "STS-B seed_86 {'other': 0.5313194444444445, 'vertical': 0.18145833333333333, 'block': 0.12909722222222222, 'mix': 0.11388888888888889, 'diagonal': 0.04423611111111111}\n",
      "STS-B seed_71 {'other': 0.598125, 'vertical': 0.13625, 'block': 0.12194444444444444, 'mix': 0.10277777777777777, 'diagonal': 0.04090277777777778}\n",
      "STS-B seed_166 {'other': 0.6440972222222222, 'block': 0.13902777777777778, 'vertical': 0.09055555555555556, 'mix': 0.085, 'diagonal': 0.04131944444444444}\n",
      "QQP seed_1337 {'other': 0.6780555555555555, 'block': 0.12, 'vertical': 0.078125, 'mix': 0.07430555555555556, 'diagonal': 0.04951388888888889}\n",
      "QQP seed_42 {'other': 0.6611805555555555, 'block': 0.10354166666666667, 'vertical': 0.09833333333333333, 'mix': 0.09381944444444444, 'diagonal': 0.043125}\n",
      "QQP seed_86 {'other': 0.653125, 'block': 0.14208333333333334, 'vertical': 0.081875, 'mix': 0.07611111111111112, 'diagonal': 0.04680555555555556}\n",
      "QQP seed_71 {'other': 0.6804166666666667, 'block': 0.11625, 'mix': 0.08333333333333333, 'vertical': 0.08180555555555556, 'diagonal': 0.03819444444444445}\n",
      "QQP seed_166 {'other': 0.6576388888888889, 'block': 0.11444444444444445, 'mix': 0.09333333333333334, 'vertical': 0.091875, 'diagonal': 0.042708333333333334}\n",
      "MNLI seed_1337 {'other': 0.5061111111111111, 'mix': 0.15819444444444444, 'vertical': 0.15416666666666667, 'block': 0.11895833333333333, 'diagonal': 0.06256944444444444}\n",
      "MNLI seed_42 {'other': 0.5521527777777778, 'mix': 0.1361111111111111, 'vertical': 0.12993055555555555, 'block': 0.11993055555555555, 'diagonal': 0.061875}\n",
      "MNLI seed_86 {'other': 0.5481944444444444, 'mix': 0.15666666666666668, 'vertical': 0.1270138888888889, 'block': 0.10881944444444444, 'diagonal': 0.059305555555555556}\n",
      "MNLI seed_71 {'other': 0.5307638888888889, 'mix': 0.16090277777777778, 'vertical': 0.12819444444444444, 'block': 0.12541666666666668, 'diagonal': 0.05472222222222222}\n",
      "MNLI seed_166 {'other': 0.4791666666666667, 'mix': 0.18618055555555554, 'vertical': 0.15881944444444446, 'block': 0.11541666666666667, 'diagonal': 0.06041666666666667}\n",
      "QNLI seed_1337 {'other': 0.41097222222222224, 'vertical': 0.21347222222222223, 'mix': 0.21166666666666667, 'block': 0.08888888888888889, 'diagonal': 0.075}\n",
      "QNLI seed_42 {'other': 0.41715277777777776, 'mix': 0.21506944444444445, 'vertical': 0.20381944444444444, 'block': 0.09416666666666666, 'diagonal': 0.06979166666666667}\n",
      "QNLI seed_86 {'other': 0.4174305555555556, 'mix': 0.22486111111111112, 'vertical': 0.20055555555555554, 'block': 0.09208333333333334, 'diagonal': 0.06506944444444444}\n",
      "QNLI seed_71 {'other': 0.393125, 'mix': 0.22618055555555555, 'vertical': 0.21472222222222223, 'block': 0.09395833333333334, 'diagonal': 0.07201388888888889}\n",
      "QNLI seed_166 {'other': 0.40881944444444446, 'mix': 0.22027777777777777, 'vertical': 0.1986111111111111, 'block': 0.10541666666666667, 'diagonal': 0.066875}\n",
      "RTE seed_1337 {'other': 0.415, 'mix': 0.2704166666666667, 'vertical': 0.175, 'block': 0.0907638888888889, 'diagonal': 0.04881944444444444}\n",
      "RTE seed_42 {'other': 0.4217361111111111, 'mix': 0.2552083333333333, 'vertical': 0.17527777777777778, 'block': 0.09881944444444445, 'diagonal': 0.04895833333333333}\n",
      "RTE seed_86 {'other': 0.4378472222222222, 'mix': 0.26069444444444445, 'vertical': 0.1698611111111111, 'block': 0.08479166666666667, 'diagonal': 0.04680555555555556}\n",
      "RTE seed_71 {'other': 0.3844444444444444, 'mix': 0.2890972222222222, 'vertical': 0.16381944444444443, 'block': 0.10673611111111111, 'diagonal': 0.05590277777777778}\n",
      "RTE seed_166 {'other': 0.41118055555555555, 'mix': 0.29, 'vertical': 0.15145833333333333, 'block': 0.094375, 'diagonal': 0.05298611111111111}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained(model_path,  config=config)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        layer_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            _, _, attentions = transformer_model(**input_data)\n",
    "            for layer in range(len(attentions)):\n",
    "                if attentions[layer] is None:\n",
    "                    continue\n",
    "                head_attentions = attentions[layer].transpose(0, 1)\n",
    "                logits = head_classifier_model(head_attentions)\n",
    "                label_ids = torch.argmax(logits, dim=-1)\n",
    "                labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                if len(layer_head_types[layer]) == 0:\n",
    "                    for i in range(len(labels)):\n",
    "                        c = Counter()\n",
    "                        layer_head_types[layer].append(c)\n",
    "                for i, label in enumerate(labels):\n",
    "                    layer_head_types[layer][i][label] += 1\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        total_counter = Counter()\n",
    "        for layer in layer_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                total_counter += head_type_ctr\n",
    "        print(task, seed, {k:v/sum(total_counter.values()) for k,v in total_counter.most_common()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model All Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoLA seed_1337 {'vertical': 0.53125, 'block': 0.3161805555555556, 'other': 0.14180555555555555, 'mix': 0.008125, 'diagonal': 0.002638888888888889}\n",
      "CoLA seed_42 {'vertical': 0.5222916666666667, 'block': 0.3507638888888889, 'other': 0.11291666666666667, 'mix': 0.01125, 'diagonal': 0.002777777777777778}\n",
      "CoLA seed_86 {'vertical': 0.4957638888888889, 'block': 0.35118055555555555, 'other': 0.14118055555555556, 'mix': 0.009305555555555555, 'diagonal': 0.0025694444444444445}\n",
      "CoLA seed_71 {'vertical': 0.46805555555555556, 'block': 0.34833333333333333, 'other': 0.17881944444444445, 'mix': 0.0034027777777777776, 'diagonal': 0.001388888888888889}\n",
      "CoLA seed_166 {'vertical': 0.48291666666666666, 'block': 0.3452777777777778, 'other': 0.16666666666666666, 'mix': 0.0044444444444444444, 'diagonal': 0.0006944444444444445}\n",
      "SST-2 seed_1337 {'vertical': 0.3767361111111111, 'other': 0.30444444444444446, 'mix': 0.1540277777777778, 'block': 0.14027777777777778, 'diagonal': 0.02451388888888889}\n",
      "SST-2 seed_42 {'vertical': 0.3932638888888889, 'other': 0.26472222222222225, 'mix': 0.1711111111111111, 'block': 0.1451388888888889, 'diagonal': 0.025763888888888888}\n",
      "SST-2 seed_86 {'vertical': 0.3984722222222222, 'other': 0.3001388888888889, 'mix': 0.14347222222222222, 'block': 0.13569444444444445, 'diagonal': 0.022222222222222223}\n",
      "SST-2 seed_71 {'vertical': 0.4222222222222222, 'other': 0.2548611111111111, 'mix': 0.17326388888888888, 'block': 0.12493055555555556, 'diagonal': 0.024722222222222222}\n",
      "SST-2 seed_166 {'vertical': 0.38416666666666666, 'other': 0.2845138888888889, 'mix': 0.19090277777777778, 'block': 0.113125, 'diagonal': 0.027291666666666665}\n",
      "MRPC seed_1337 {'vertical': 0.3507638888888889, 'other': 0.27875, 'mix': 0.23791666666666667, 'block': 0.086875, 'diagonal': 0.04569444444444445}\n",
      "MRPC seed_42 {'vertical': 0.3713194444444444, 'mix': 0.25298611111111113, 'other': 0.24069444444444443, 'block': 0.08923611111111111, 'diagonal': 0.04576388888888889}\n",
      "MRPC seed_86 {'vertical': 0.3692361111111111, 'other': 0.24708333333333332, 'mix': 0.24409722222222222, 'block': 0.09145833333333334, 'diagonal': 0.048125}\n",
      "MRPC seed_71 {'vertical': 0.3609027777777778, 'other': 0.25069444444444444, 'mix': 0.24861111111111112, 'block': 0.09166666666666666, 'diagonal': 0.048125}\n",
      "MRPC seed_166 {'vertical': 0.3579861111111111, 'other': 0.27493055555555557, 'mix': 0.23291666666666666, 'block': 0.08805555555555555, 'diagonal': 0.04611111111111111}\n",
      "STS-B seed_1337 {'other': 0.5999305555555555, 'vertical': 0.1613888888888889, 'block': 0.10465277777777778, 'mix': 0.10263888888888889, 'diagonal': 0.03138888888888889}\n",
      "STS-B seed_42 {'other': 0.6326388888888889, 'vertical': 0.12979166666666667, 'block': 0.1111111111111111, 'mix': 0.09833333333333333, 'diagonal': 0.028125}\n",
      "STS-B seed_86 {'other': 0.6127777777777778, 'vertical': 0.14923611111111112, 'block': 0.105625, 'mix': 0.10402777777777777, 'diagonal': 0.028333333333333332}\n",
      "STS-B seed_71 {'other': 0.6092361111111111, 'vertical': 0.15104166666666666, 'block': 0.10770833333333334, 'mix': 0.10319444444444445, 'diagonal': 0.028819444444444446}\n",
      "STS-B seed_166 {'other': 0.6535416666666667, 'vertical': 0.12944444444444445, 'block': 0.10972222222222222, 'mix': 0.08083333333333333, 'diagonal': 0.026458333333333334}\n",
      "QQP seed_1337 {'other': 0.6720138888888889, 'vertical': 0.11756944444444445, 'block': 0.09576388888888888, 'mix': 0.08326388888888889, 'diagonal': 0.03138888888888889}\n",
      "QQP seed_42 {'other': 0.6463194444444444, 'vertical': 0.13409722222222223, 'mix': 0.09680555555555556, 'block': 0.08916666666666667, 'diagonal': 0.03361111111111111}\n",
      "QQP seed_86 {'other': 0.6626388888888889, 'vertical': 0.11701388888888889, 'block': 0.10645833333333334, 'mix': 0.08423611111111111, 'diagonal': 0.029652777777777778}\n",
      "QQP seed_71 {'other': 0.6765972222222222, 'vertical': 0.11180555555555556, 'block': 0.09993055555555555, 'mix': 0.08006944444444444, 'diagonal': 0.03159722222222222}\n",
      "QQP seed_166 {'other': 0.6414583333333334, 'vertical': 0.14027777777777778, 'mix': 0.09326388888888888, 'block': 0.09125, 'diagonal': 0.03375}\n",
      "MNLI seed_1337 {'other': 0.5193055555555556, 'vertical': 0.20194444444444445, 'mix': 0.1457638888888889, 'block': 0.09645833333333333, 'diagonal': 0.03652777777777778}\n",
      "MNLI seed_42 {'other': 0.5603472222222222, 'vertical': 0.1696527777777778, 'mix': 0.13534722222222223, 'block': 0.1, 'diagonal': 0.034652777777777775}\n",
      "MNLI seed_86 {'other': 0.5604861111111111, 'vertical': 0.1726388888888889, 'mix': 0.14118055555555556, 'block': 0.08854166666666667, 'diagonal': 0.03715277777777778}\n",
      "MNLI seed_71 {'other': 0.5483333333333333, 'vertical': 0.18020833333333333, 'mix': 0.13583333333333333, 'block': 0.10277777777777777, 'diagonal': 0.03284722222222222}\n",
      "MNLI seed_166 {'other': 0.5175694444444444, 'vertical': 0.18590277777777778, 'mix': 0.16458333333333333, 'block': 0.09493055555555556, 'diagonal': 0.03701388888888889}\n",
      "QNLI seed_1337 {'other': 0.3475, 'vertical': 0.31840277777777776, 'mix': 0.19027777777777777, 'block': 0.09388888888888888, 'diagonal': 0.049930555555555554}\n",
      "QNLI seed_42 {'other': 0.3485416666666667, 'vertical': 0.30791666666666667, 'mix': 0.19152777777777777, 'block': 0.10048611111111111, 'diagonal': 0.051527777777777777}\n",
      "QNLI seed_86 {'other': 0.36319444444444443, 'vertical': 0.3013888888888889, 'mix': 0.18722222222222223, 'block': 0.09784722222222222, 'diagonal': 0.050347222222222224}\n",
      "QNLI seed_71 {'other': 0.33826388888888886, 'vertical': 0.32222222222222224, 'mix': 0.19479166666666667, 'block': 0.0948611111111111, 'diagonal': 0.04986111111111111}\n",
      "QNLI seed_166 {'other': 0.3481944444444444, 'vertical': 0.30895833333333333, 'mix': 0.19368055555555555, 'block': 0.10041666666666667, 'diagonal': 0.04875}\n",
      "RTE seed_1337 {'other': 0.39222222222222225, 'mix': 0.2479861111111111, 'vertical': 0.20833333333333334, 'block': 0.10020833333333333, 'diagonal': 0.05125}\n",
      "RTE seed_42 {'other': 0.40618055555555554, 'mix': 0.24055555555555555, 'vertical': 0.2070138888888889, 'block': 0.096875, 'diagonal': 0.049375}\n",
      "RTE seed_86 {'other': 0.4170833333333333, 'mix': 0.24048611111111112, 'vertical': 0.19451388888888888, 'block': 0.09854166666666667, 'diagonal': 0.049375}\n",
      "RTE seed_71 {'other': 0.3770833333333333, 'mix': 0.25881944444444444, 'vertical': 0.20875, 'block': 0.101875, 'diagonal': 0.05347222222222222}\n",
      "RTE seed_166 {'other': 0.38333333333333336, 'mix': 0.2578472222222222, 'vertical': 0.20875, 'block': 0.09805555555555556, 'diagonal': 0.05201388888888889}\n"
     ]
    }
   ],
   "source": [
    "set_seed(1337)\n",
    "for task in [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI\", \"QNLI\", \"RTE\"]:\n",
    "    for seed in [\"seed_1337\", \"seed_42\", \"seed_86\", \"seed_71\", \"seed_166\"]:\n",
    "        #Load Model\n",
    "        model_path = f\"../models/finetuned/{task}/{seed}/\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        config = BertConfig.from_pretrained(model_path)\n",
    "        config.output_attentions = True\n",
    "        transformer_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',  config=config)\n",
    "        transformer_model = transformer_model.eval()\n",
    "        transformer_model.cuda()\n",
    "        args = Namespace(data_dir=f\"../data/glue/{task}/\", local_rank=-1, \n",
    "                         model_name_or_path=model_path, \n",
    "                         overwrite_cache=False, model_type=\"bert\", max_seq_length=128)\n",
    "        eval_dataset = load_and_cache_examples(args, task.lower(), tokenizer, evaluate=True)\n",
    "        eval_sampler = RandomSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "        input_data = None\n",
    "        layer_head_types = [[] for _ in range(12)]\n",
    "\n",
    "        k = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(\"cuda:0\") for t in batch)\n",
    "            input_data =  {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            _, _, attentions = transformer_model(**input_data)\n",
    "            for layer in range(len(attentions)):\n",
    "                if attentions[layer] is None:\n",
    "                    continue\n",
    "                head_attentions = attentions[layer].transpose(0, 1)\n",
    "                logits = head_classifier_model(head_attentions)\n",
    "                label_ids = torch.argmax(logits, dim=-1)\n",
    "                labels = [id2label[int(label_id.item())] for label_id in label_ids]\n",
    "                if len(layer_head_types[layer]) == 0:\n",
    "                    for i in range(len(labels)):\n",
    "                        c = Counter()\n",
    "                        layer_head_types[layer].append(c)\n",
    "                for i, label in enumerate(labels):\n",
    "                    layer_head_types[layer][i][label] += 1\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                break\n",
    "        total_counter = Counter()\n",
    "        for layer in layer_head_types:\n",
    "            for head_type_ctr in layer:\n",
    "                total_counter += head_type_ctr\n",
    "        print(task, seed, {k:v/sum(total_counter.values()) for k,v in total_counter.most_common()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
